{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from imblearn.over_sampling import SMOTE \n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "test_data = pd.read_csv('/Users/kamayanirai/Downloads/test.csv (1).zip')\n",
    "train_data = pd.read_csv('/Users/kamayanirai/Downloads/train.csv (1).zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          0\n",
       "cont1       0\n",
       "cont2       0\n",
       "cont3       0\n",
       "cont4       0\n",
       "           ..\n",
       "cat116_U    0\n",
       "cat116_V    0\n",
       "cat116_W    0\n",
       "cat116_X    0\n",
       "cat116_Y    0\n",
       "Length: 1192, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125546, 1192)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(188318, 1192)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          0\n",
       "cont1       0\n",
       "cont2       0\n",
       "cont3       0\n",
       "cont4       0\n",
       "           ..\n",
       "cat116_U    0\n",
       "cat116_V    0\n",
       "cat116_W    0\n",
       "cat116_X    0\n",
       "cat116_Y    0\n",
       "Length: 1192, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X = train_data.drop(['loss', 'id'], axis=1)\n",
    "y = train_data['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          int64\n",
       "cat1       object\n",
       "cat2       object\n",
       "cat3       object\n",
       "cat4       object\n",
       "           ...   \n",
       "cont11    float64\n",
       "cont12    float64\n",
       "cont13    float64\n",
       "cont14    float64\n",
       "loss      float64\n",
       "Length: 132, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          int64\n",
       "cat1       object\n",
       "cat2       object\n",
       "cat3       object\n",
       "cat4       object\n",
       "           ...   \n",
       "cont10    float64\n",
       "cont11    float64\n",
       "cont12    float64\n",
       "cont13    float64\n",
       "cont14    float64\n",
       "Length: 131, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with object dtype: Index(['cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8', 'cat9',\n",
      "       'cat10',\n",
      "       ...\n",
      "       'cat107', 'cat108', 'cat109', 'cat110', 'cat111', 'cat112', 'cat113',\n",
      "       'cat114', 'cat115', 'cat116'],\n",
      "      dtype='object', length=116)\n"
     ]
    }
   ],
   "source": [
    "# Get all columns with dtype 'object'\n",
    "object_columns = train_data.select_dtypes(include=['object']).columns\n",
    "print(\"Columns with object dtype:\", object_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine train and test data for consistent one-hot encoding\n",
    "combined_data = pd.concat([train_data, test_data], keys=[0, 1])\n",
    "\n",
    "# List of categorical columns for one-hot encoding\n",
    "categorical_features = [col for col in combined_data.columns if \"cat\" in col]\n",
    "\n",
    "# Perform one-hot encoding\n",
    "combined_data = pd.get_dummies(combined_data, columns=categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate combined data back into train and test sets\n",
    "train_data = combined_data.xs(0)\n",
    "test_data = combined_data.xs(1)\n",
    "\n",
    "# Extract target variable and drop unnecessary columns\n",
    "y = train_data['loss']\n",
    "X = train_data.drop(['loss', 'id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kamayanirai/anaconda3/envs/Machine_Learning/lib/python3.8/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 4 is smaller than n_iter=5. Running 4 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Decision Tree: {'min_samples_split': 10, 'max_depth': 10}\n",
      "RMSE (training) for Decision Tree: 1991.962836\n",
      "RMSE (Test Data) for Decision Tree: 2073.090850\n"
     ]
    }
   ],
   "source": [
    "# Train-validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Hyperparameter tuning for DecisionTreeRegressor using RandomizedSearchCV\n",
    "param_dist_dt = {\n",
    "    'max_depth': [10, 15],\n",
    "    'min_samples_split': [5, 10],\n",
    "   # 'min_samples_leaf': [2, 4],\n",
    "   # 'criterion': ['squared_error']\n",
    "}\n",
    "random_search_dt = RandomizedSearchCV(DecisionTreeRegressor(random_state=42), param_dist_dt,n_iter=5, cv=3, scoring='neg_mean_squared_error', n_jobs=-1, random_state=42)\n",
    "random_search_dt.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and model for DecisionTreeRegressor\n",
    "print(f\"Best parameters for Decision Tree: {random_search_dt.best_params_}\")\n",
    "best_dt = random_search_dt.best_estimator_\n",
    "\n",
    "# Evaluate the best DecisionTreeRegressor model\n",
    "dt_predict_Train = best_dt.predict(X_train)\n",
    "rmse_train_dt = np.sqrt(mean_squared_error(y_train, dt_predict_Train))\n",
    "print(\"RMSE (training) for Decision Tree: {0:10f}\".format(rmse_train_dt))\n",
    "\n",
    "dt_predict_Test = best_dt.predict(X_val)\n",
    "rmse_test_dt = np.sqrt(mean_squared_error(y_val, dt_predict_Test))\n",
    "print(\"RMSE (Test Data) for Decision Tree: {0:10f}\".format(rmse_test_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Predictions saved to 'dt_predictions_tuned.csv'.\n"
     ]
    }
   ],
   "source": [
    "X_test = test_data.drop(['id'], axis=1)  # Drop the 'id' column from the test data\n",
    "X_test = X_test[X_train.columns]  # Align columns of X_test with X_train columns\n",
    "\n",
    "# Make predictions for best models on test data\n",
    "dt_predict_Test = best_dt.predict(X_test)\n",
    "\n",
    "# Save predictions for the model to CSV files\n",
    "df_dt = pd.DataFrame({'ID': test_data['id'], 'loss': dt_predict_Test})\n",
    "df_dt.to_csv('/Users/kamayanirai/Downloads/output1/dt_predictions_tuned.csv', index=False)\n",
    "print(\"Decision Tree Predictions saved to 'dt_predictions_tuned.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified Sampling\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_idx, val_idx in split.split(train_data, pd.qcut(train_data['loss'], q=5, duplicates='drop')):  \n",
    "    feature_selection_data = train_data.iloc[train_idx]\n",
    "    validation_data = train_data.iloc[val_idx]\n",
    "\n",
    "# Step 2: Feature and Target Extraction\n",
    "# For feature selection\n",
    "X_train_fs = feature_selection_data.drop(['loss', 'id'], axis=1)\n",
    "y_train_fs = feature_selection_data['loss']\n",
    "\n",
    "# For validation\n",
    "X_val = validation_data.drop(['loss', 'id'], axis=1)\n",
    "y_val = validation_data['loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Feature Selection for Decision Tre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 features by importance:\n",
      " Index(['cat80_B', 'cat57_A', 'cont7', 'cat79_D', 'cat12_B', 'cont2', 'cat81_D',\n",
      "       'cat1_A', 'cont12', 'cat53_A'],\n",
      "      dtype='object')\n",
      "RMSE (Training) for Decision Tree with top features: 1323.339804\n",
      "RMSE (Validation) for Decision Tree with top features: 2754.754259\n"
     ]
    }
   ],
   "source": [
    " #Training a Decision Tree Regressor for Feature Selection\n",
    "clf = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
    "clf.fit(X_train_fs, y_train_fs)\n",
    "\n",
    "# Feature Importances\n",
    "importances = clf.feature_importances_\n",
    "sorted_indices = np.argsort(importances)[::-1]  # Descending order\n",
    "top_n = 10 \n",
    "top_features = X_train_fs.columns[sorted_indices[:top_n]]\n",
    "\n",
    "print(f\"\\nTop {top_n} features by importance:\\n\", top_features)\n",
    "\n",
    "# Select Top Features for Training and Validation\n",
    "X_train_fs = X_train_fs[top_features]\n",
    "X_val_fs = X_val[top_features]\n",
    "\n",
    "# Train and Evaluate Final Model\n",
    "clf = DecisionTreeRegressor(random_state=42)\n",
    "clf.fit(X_train_fs, y_train_fs)\n",
    "\n",
    "# Predictions on training data\n",
    "clf_predict_train = clf.predict(X_train_fs)\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train_fs, clf_predict_train))\n",
    "print(f\"RMSE (Training) for Decision Tree with top features: {rmse_train:.6f}\")\n",
    "\n",
    "# Predictions on validation data\n",
    "clf_predict_val = clf.predict(X_val_fs)\n",
    "rmse_val = np.sqrt(mean_squared_error(y_val, clf_predict_val))\n",
    "print(f\"RMSE (Validation) for Decision Tree with top features: {rmse_val:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Training) for Random Forest: 2068.710445\n",
      "RMSE (Validation) for Random Forest: 2102.489282\n",
      "Random Forest Predictions saved to 'rf_predictions_tuned.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Defining and train the Random Forest Regressor\n",
    "rfc = RandomForestRegressor(\n",
    "    n_estimators=300, \n",
    "    max_depth=10,  \n",
    "    max_features='sqrt',\n",
    "    n_jobs=-1,  \n",
    "    random_state=42\n",
    ")\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and RMSE on training data\n",
    "rfc_predict_Train = rfc.predict(X_train)\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, rfc_predict_Train))\n",
    "print(f\"RMSE (Training) for Random Forest: {rmse_train:.6f}\")\n",
    "\n",
    "# Predictions and RMSE on validation data\n",
    "rfc_predict_Val = rfc.predict(X_val)\n",
    "rmse_val = np.sqrt(mean_squared_error(y_val, rfc_predict_Val))\n",
    "print(f\"RMSE (Validation) for Random Forest: {rmse_val:.6f}\")\n",
    "\n",
    "# Predictions on test data\n",
    "X_test_fs = X_test[X_train.columns] \n",
    "rfc_predict_Test = rfc.predict(X_test_fs)\n",
    "\n",
    "# Save predictions for test data\n",
    "df_rf = pd.DataFrame({'ID': test_data['id'], 'loss': rfc_predict_Test})\n",
    "df_rf.to_csv('/Users/kamayanirai/Downloads/output1/rf_predictions_tuned.csv', index=False)\n",
    "print(\"Random Forest Predictions saved to 'rf_predictions_tuned.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Feature Selection for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 features by importance:\n",
      " Index(['cont7', 'cat80_B', 'cont12', 'cont2', 'cat57_A', 'cat79_D', 'cat12_B',\n",
      "       'cat81_D', 'cat1_A', 'cat53_A'],\n",
      "      dtype='object')\n",
      "RMSE (Training) for Random Forest with top features: 1438.122217\n",
      "RMSE (Validation) for Random Forest with top features: 2289.821045\n"
     ]
    }
   ],
   "source": [
    "#Training a random forest Regressor for Feature Selection\n",
    "rfc = RandomForestRegressor()\n",
    "rfc.fit(X_train_fs, y_train_fs)\n",
    "\n",
    "# Feature Importances\n",
    "importances = rfc.feature_importances_\n",
    "sorted_indices = np.argsort(importances)[::-1]  # Descending order\n",
    "top_n = 10 \n",
    "top_features = X_train_fs.columns[sorted_indices[:top_n]]\n",
    "\n",
    "print(f\"\\nTop {top_n} features by importance:\\n\", top_features)\n",
    "\n",
    "#Select Top Features for Training and Validation\n",
    "X_train_fs = X_train_fs[top_features]\n",
    "X_val_fs = X_val[top_features]\n",
    "\n",
    "#Train and Evaluate Final Model\n",
    "rfc = RandomForestRegressor(random_state=42)\n",
    "rfc.fit(X_train_fs, y_train_fs)\n",
    "\n",
    "# Predictions on training data\n",
    "rfc_predict_train = rfc.predict(X_train_fs)\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train_fs, rfc_predict_train))\n",
    "print(f\"RMSE (Training) for Random Forest with top features: {rmse_train:.6f}\")\n",
    "\n",
    "# Predictions on validation data\n",
    "rfc_predict_val = rfc.predict(X_val_fs)\n",
    "rmse_val = np.sqrt(mean_squared_error(y_val, rfc_predict_val))\n",
    "print(f\"RMSE (Validation) for Random Forest with top features: {rmse_val:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Training) for Gradient Boosting: 1923.054091\n",
      "RMSE (Validation) for Gradient Boosting: 1937.852621\n",
      "Gradient Boosting Predictions saved to 'gbr_predictions_tuned.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Define and train the Gradient Boosting Regressor\n",
    "gbr = GradientBoostingRegressor(\n",
    "    n_estimators=100, \n",
    "    max_depth=3, \n",
    "    learning_rate=0.1, \n",
    "    random_state=42\n",
    ")\n",
    "gbr.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and RMSE on training data\n",
    "gbr_predict_Train = gbr.predict(X_train)\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, gbr_predict_Train))\n",
    "print(f\"RMSE (Training) for Gradient Boosting: {rmse_train:.6f}\")\n",
    "\n",
    "# Predictions and RMSE on validation data\n",
    "gbr_predict_Val = gbr.predict(X_val)\n",
    "rmse_val = np.sqrt(mean_squared_error(y_val, gbr_predict_Val))\n",
    "print(f\"RMSE (Validation) for Gradient Boosting: {rmse_val:.6f}\")\n",
    "\n",
    "# Predictions on test data\n",
    "X_test_fs = X_test[X_train.columns] \n",
    "gbr_predict_Test = gbr.predict(X_test_fs)\n",
    "\n",
    "# Save predictions for test data\n",
    "df_gbr = pd.DataFrame({'ID': test_data['id'], 'loss': gbr_predict_Test})\n",
    "df_gbr.to_csv('/Users/kamayanirai/Downloads/output1/gbr_predictions_tuned.csv', index=False)\n",
    "print(\"Gradient Boosting Predictions saved to 'gbr_predictions_tuned.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Feature Selection for Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 features by importance:\n",
      " Index(['cat80_B', 'cat79_D', 'cont7', 'cat57_A', 'cat12_B', 'cont2', 'cat81_D',\n",
      "       'cont12', 'cat1_A', 'cat53_A'],\n",
      "      dtype='object')\n",
      "RMSE (Training) for Gradient Boosting with top features: 2038.500568\n",
      "RMSE (Validation) for Gradient Boosting with top features: 2144.156510\n"
     ]
    }
   ],
   "source": [
    "#Training a Gradient Boosting Regressor for Feature Selection\n",
    "gbr = GradientBoostingRegressor(random_state=42)\n",
    "gbr.fit(X_train_fs, y_train_fs)  # Fit the model first\n",
    "\n",
    "# Feature Importances\n",
    "importances = gbr.feature_importances_  # Access feature importances after fitting\n",
    "sorted_indices = np.argsort(importances)[::-1]  # Descending order\n",
    "top_n = 10  # Number of top features to select\n",
    "top_features = X_train_fs.columns[sorted_indices[:top_n]]\n",
    "\n",
    "print(f\"\\nTop {top_n} features by importance:\\n\", top_features)\n",
    "\n",
    "# Select Top Features for Training and Validation\n",
    "X_train_fs = X_train_fs[top_features]\n",
    "X_val_fs = X_val[top_features]\n",
    "\n",
    "# Train and Evaluate Final Model\n",
    "final_gbr = GradientBoostingRegressor(random_state=42)\n",
    "final_gbr.fit(X_train_fs, y_train_fs)\n",
    "\n",
    "# Predictions on training data\n",
    "gbr_predict_train = final_gbr.predict(X_train_fs)\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train_fs, gbr_predict_train))\n",
    "print(f\"RMSE (Training) for Gradient Boosting with top features: {rmse_train:.6f}\")\n",
    "\n",
    "# Predictions on validation data\n",
    "gbr_predict_val = final_gbr.predict(X_val_fs)\n",
    "rmse_val = np.sqrt(mean_squared_error(y_val, gbr_predict_val))\n",
    "print(f\"RMSE (Validation) for Gradient Boosting with top features: {rmse_val:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task:         [regression]\n",
      "metric:       [mean_absolute_error]\n",
      "mode:         [oof_pred_bag]\n",
      "n_models:     [3]\n",
      "\n",
      "model  0:     [GradientBoostingRegressor]\n",
      "    fold  0:  [1259.90416894]\n",
      "    fold  1:  [1256.31732515]\n",
      "    fold  2:  [1256.14936832]\n",
      "    ----\n",
      "    MEAN:     [1257.45695414] + [1.73180014]\n",
      "    FULL:     [1257.45695414]\n",
      "\n",
      "model  1:     [RandomForestRegressor]\n",
      "    fold  0:  [1255.20610070]\n",
      "    fold  1:  [1243.36057840]\n",
      "    fold  2:  [1248.08351498]\n",
      "    ----\n",
      "    MEAN:     [1248.88339803] + [4.86887799]\n",
      "    FULL:     [1248.88339803]\n",
      "\n",
      "model  2:     [DecisionTreeRegressor]\n",
      "    fold  0:  [1744.58055179]\n",
      "    fold  1:  [1737.58924330]\n",
      "    fold  2:  [1740.87878211]\n",
      "    ----\n",
      "    MEAN:     [1741.01619240] + [2.85584311]\n",
      "    FULL:     [1741.01619240]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from vecstack import stacking\n",
    "models = [ GradientBoostingRegressor(), RandomForestRegressor(), DecisionTreeRegressor() ]\n",
    "      \n",
    "S_Train, S_Test = stacking(models,                   \n",
    "                           X_train, y_train, X_test,   \n",
    "                           regression=True, mode='oof_pred_bag', needs_proba=False, save_dir=None, shuffle=True, \n",
    "                           random_state=42, n_folds=3, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STACKING - CONTRUCT A GRADIENT BOOSTING MODEL\n",
    "model = GradientBoostingRegressor()\n",
    "    \n",
    "model = model.fit(S_Train, y_train)\n",
    "y_pred_train = model.predict(S_Train)\n",
    "y_pred_test = model.predict(S_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Training) for Gradient Boosting (stacked model): 1900.183562\n",
      "Gradient Boosting Predictions saved to 'gbr_predictions_tuned_stacked.csv'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kamayanirai/anaconda3/envs/Machine_Learning/lib/python3.8/site-packages/sklearn/base.py:458: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Predictions and RMSE on training data\n",
    "y_pred_train = model.predict(S_Train)\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "print(f\"RMSE (Training) for Gradient Boosting (stacked model): {rmse_train:.6f}\")\n",
    "\n",
    "X_test_fs = X_test.iloc[:, :S_Train.shape[1]]  # Select the same number of features as S_Train\n",
    "gbr_predict_Test = model.predict(X_test_fs)\n",
    "\n",
    "# Save predictions for test data\n",
    "df_gbr = pd.DataFrame({'ID': test_data['id'], 'loss': gbr_predict_Test})\n",
    "df_gbr.to_csv('/Users/kamayanirai/Downloads/output1/gbr_predictions_tuned_stacked.csv', index=False)\n",
    "print(\"Gradient Boosting Predictions saved to 'gbr_predictions_tuned_stacked.csv'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Machine_Learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
